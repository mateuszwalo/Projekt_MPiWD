---
title: "Analiza katastrof globalnych"
author: "Mateusz Walo"
format: 
  html:
    toc: true                
    toc-depth: 2             
    toc-title: "Spis treści" 
    self-contained: true    
    theme: flatly            
    number-sections: true    
    fig-height: 6            
    fig-width: 8             
    fig-align: center        
    embed-resources: true    
    df-print: paged          
    include-in-header:
      text: |
        <script src="https://cdnjs.cloudflare.com/ajax/libs/plotly.js/2.20.0/plotly.min.js"></script> 
editor: visual
execute: 
  echo: false               # Ukrycie kodu w wynikowym raporcie
  warning: false            # Wyłączenie komunikatów ostrzegawczych
  message: false            # Wyłączenie komunikatów diagnostycznych
  freeze: auto              
---

## Problem badawczy

Na świecie występuje wiele zagrożeń, które mają istotny wpływ na bezpieczeństwo ludzi. Można je podzielić na dwie główne kategorie:

1.  **Zagrożenia zależne od działalności człowieka** – na przykład katastrofy ekologiczne, takie jak wycieki ropy naftowej spowodowane rozbiciem tankowców.

2.  **Zagrożenia naturalne** – takie jak susze, powodzie, huragany i inne zjawiska przyrodnicze.

### Cel analizy

Niniejsza analiza ma na celu zbadanie danych pochodzących z platformy EMDAT, aby lepiej zrozumieć charakter i skalę zagrożeń, z którymi mierzą się ludzie na całym świecie. W szczególności podjęte zostaną próby odpowiedzi na następujące pytania:

-   **Które państwo lub region jest najbezpieczniejszy do życia pod względem występowania katastrof?**

-   **Które państwo lub region jest najbardziej narażone na występowanie katastrof?**

-   **Jakie typy katastrof najczęściej występują w określonych regionach?**

-   **Jakie są skutki katastrof dla mieszkańców poszczególnych regionów?**

-   **Jak wygląda historyczny kontekst skutków występowania katastrof?**

Ponadto, planowane jest stworzenie modeli predykcyjnych, które pozwolą oszacować potencjalne skutki i straty związane z przyszłymi katastrofami.

## Opis zbioru danych

### Pochodzenie danych

Dane wykorzystywane w analizie pochodzą z platformy EMDAT, która udostępnia informacje o wszystkich zarejestrowanych katastrofach na świecie. Zakres czasowy wybranych danych obejmuje lata 1999–2024.

### Zawartość zbioru

Zbiór danych zawiera następujące informacje:

-   **Typ zdarzenia** – rodzaj katastrofy (np. powódź, huragan, trzęsienie ziemi) z podziałem na podtypy i podgatunki.

-   **Data wystąpienia** – dokładne informacje o czasie zdarzenia.

-   **Region** – lokalizacja geograficzna (kontynent, państwo, region).

-   **Skutki** – dane dotyczące strat materialnych i ludzkich.

```{r}
library(tidyverse)
library(ggplot2)
library(leaflet)
library(dplyr)
library(sf)
library(tibble)
library(knitr)
library(rnaturalearth)
library(networkD3)
library(jsonlite)
library(gganimate)

# data <- read.csv("public_emdat_custom_request_2024-10-03_fdbbae24-da6c-4137-b8d4-3c5548a622df.csv")
# head(data)
```

### Sposób pozyskania danych

Dane zostały pozyskane za pomocą funkcjonalnego API udostępnionego przez EMDAT. Proces wymagał zalogowania się na platformę oraz uzyskania odpowiedniej autoryzacji, co umożliwiło selekcję i pobranie potrzebnych informacji bezpośrednio z systemu EMDAT.

## Plan analizy

W dalszych częściach raportu zostaną szczegółowo omówione zmienne zawarte w zbiorze danych, a także przeprowadzone zostaną analizy odpowiadające na wyżej wymienione pytania badawcze. Szczególna uwaga zostanie poświęcona:

-   Identyfikacji trendów w występowaniu katastrof.

-   Regionalnemu rozkładowi zagrożeń i ich skutków.

-   Oceny możliwości predykcji rodzajów katastrof.

Celem końcowym jest wypracowanie wniosków, które mogą wspierać strategie ograniczania skutków katastrof na poziomie lokalnym i globalnym.

### Preprocessing i czyszczenie danych

Z uwagi na fakt że surowe dane nie są przedstawione w przystępnej formie, w związku z tym do dalejszej analizy wybierzemy z danych te kolumny które mają dla nas kluczową wartość i oczyścimy je do poziomu istotnych dla nas pred

```{python}
# import pandas as pd
# 
# file_path = 'public_emdat_custom_request_2024-10-03_fdbbae24-da6c-4137-b8d4-3c5548a622df.xlsx'
# 
# raw_data = pd.ExcelFile(file_path)
# raw_data = raw_data.parse('EM-DAT Data')
# 
# key_columns = [
#     'Classification Key', 'Disaster Subtype', 'Disaster Subgroup', 'Disaster Group','Disaster Type', 'Country', 'Region', 'Start Year', 'Start Month', 
#     'End Year', 'End Month', 'Total Deaths', 'No. Injured', 'Total Affected', 
#     'Total Damage (\'000 US$)', 'Latitude', 'Longitude'
# ]
# cleaned_data = raw_data[key_columns]
# 
# numerical_columns = ['Total Deaths', 'No. Injured', 'Total Affected', 'Total Damage (\'000 US$)', 'Latitude', 'Longitude']
# cleaned_data[numerical_columns] = cleaned_data[numerical_columns].fillna(0)
# 
# categorical_columns = [
#     'Disaster Type', 'Country', 'Region', 'Disaster Subtype', 
#     'Disaster Subgroup', 'Disaster Group', 'Classification Key'
# ]
# cleaned_data[categorical_columns] = cleaned_data[categorical_columns].fillna('Unknown')
# 
# cleaned_data['Start Date'] = pd.to_datetime(
#     cleaned_data['Start Year'].astype(str) + '-' +
#     cleaned_data['Start Month'].fillna(1).astype(int).astype(str) + '-01',
#     errors='coerce'
# )
# cleaned_data['End Date'] = pd.to_datetime(
#     cleaned_data['End Year'].astype(str) + '-' +
#     cleaned_data['End Month'].fillna(1).astype(int).astype(str) + '-01',
#     errors='coerce'
# )
# 
# simplified_data = cleaned_data[
#     ['Classification Key', 'Disaster Group', 'Disaster Subgroup', 'Disaster Subtype', 
#      'Disaster Type', 'Country', 'Region', 'Start Date', 'End Date', 
#      'Total Deaths', 'No. Injured', 'Total Affected', 'Total Damage (\'000 US$)', 
#      'Latitude', 'Longitude']
# ]
# 
# output_path = 'C:\\STUDIA MATERIAŁY\\Semestr III\\MPiWD\\Projekt_MPiWD_Walo_Mateusz\\simplified_data.csv'
# simplified_data.to_csv(output_path, index=False)
```

```{r}
data <- read.csv("simplified_data.csv")
head(data)
```

## Wizualizacje zbioru danych

```         
```

**Rodzaje katastrof wraz z ich liczebnością i podziałem na typ, kategorie i podkategorię**

```{python}
import plotly.graph_objects as go

ids = [
    "Natural", "Technological", 
    "Natural - Climatological", "Natural - Hydrological", "Natural - Meteorological", 
    "Natural - Geophysical", "Technological - Transport", "Technological - Miscellaneous accident", 
    "Technological - Industrial accident", "Technological - Biological", "Technological - Extra-terrestrial",
    "Natural - Climatological - Drought", "Technological - Transport - Road", "Natural - Hydrological - Flood",
    "Natural - Meteorological - Extreme temperature", "Technological - Miscellaneous accident - Fire (Miscellaneous)",
    "Natural - Geophysical - Volcanic activity", "Natural - Meteorological - Storm", "Natural - Meteorological - Wildfire",
    "Natural - Geophysical - Earthquake", "Technological - Transport - Rail", "Technological - Transport - Air",
    "Technological - Industrial accident - Collapse (Industrial)", "Technological - Industrial accident - Collapse (Miscellaneous)",
    "Technological - Industrial accident - Explosion (Miscellaneous)", "Technological - Biological - Epidemic",
    "Technological - Biological - Water", "Technological - Biological - Mass movement (wet)", "Technological - Biological - Explosion (Industrial)",
    "Technological - Biological - Chemical spill", "Technological - Biological - Gas leak", "Technological - Biological - Infestation",
    "Technological - Biological - Miscellaneous accident (General)", "Technological - Biological - Poisoning",
    "Technological - Biological - Mass movement (dry)", "Technological - Industrial accident (General) - Radiation",
    "Technological - Industrial accident (General) - Oil spill", "Technological - Industrial accident (General) - Impact",
    "Technological - Industrial accident (General) - Animal incident", "Technological - Industrial accident (General) - Glacial lake outburst flood"
]

labels = [
    "Natural", "Technological", 
    "Climatological", "Hydrological", "Meteorological", 
    "Geophysical", "Transport", "Miscellaneous accident", 
    "Industrial accident", "Biological", "Extra-terrestrial",
    "Drought", "Road", "Flood", "Extreme temperature", "Fire (Miscellaneous)",
    "Volcanic activity", "Storm", "Wildfire", "Earthquake", "Rail", "Air",
    "Collapse (Industrial)", "Collapse (Miscellaneous)", "Explosion (Miscellaneous)", "Epidemic",
    "Water", "Mass movement (wet)", "Explosion (Industrial)", "Chemical spill", "Gas leak", "Infestation",
    "Miscellaneous accident (General)", "Poisoning", "Mass movement (dry)", "Radiation", "Oil spill", "Impact",
    "Animal incident", "Glacial lake outburst flood"
]

parents = [
    "", "", "Natural", "Natural", "Natural", "Natural", "Technological", "Technological", 
    "Technological", "Technological", "Technological",
    "Natural - Climatological", "Technological - Transport", "Natural - Hydrological", "Natural - Meteorological",
    "Technological - Miscellaneous accident", "Natural - Geophysical", "Natural - Meteorological", "Natural - Meteorological",
    "Natural - Geophysical", "Technological - Transport", "Technological - Transport", "Technological - Industrial accident",
    "Technological - Industrial accident", "Technological - Industrial accident", "Technological - Biological",
    "Technological - Biological", "Technological - Biological", "Technological - Biological", "Technological - Biological",
    "Technological - Biological", "Technological - Biological", "Technological - Biological", "Technological - Biological",
    "Technological - Biological", "Technological - Biological", "Technological - Biological", "Technological - Biological"
]

fig = go.Figure(go.Sunburst(
    ids=ids,
    labels=labels,
    parents=parents,
))

fig.update_layout(
    margin=dict(t=20, l=20, r=20, b=20),
    width=900,
    height=500
)
```

**Wnioski do wizualizacji:**

-   Większość aktualnych zagrożeń wynika z obszaru technologicznegio-\> Człowiek powoduje samoistnie katastrofy klimatyczne, wpływając tym też na zagrożenia naturalne, tworząc efekty ekologiczne negatywne dla planety, np. efekt cieplarniany, dziury ozonowe etc.

-   Na przetrzeni 25 lat, znacząco częśc katastrof były biologiczne u któych podstaw była technologia!

*Komentarz:*

Na podstawie tej wizualizacji firmy ubezpieczeniowe mogłyby dostosować swoją ofertę do ubezpieczeń majątkowych, ustalając konrektene kwoty np. w danym państwie jeśli częścią wustępują tam powodzie niż pożary etc. Świadomość społeczna tego że ludzkie działania przyczyniają się do wyniczania planety może wiele zmienić, co możemy dokładnie zobaczyć na wyresie

```{r}
data <- data %>%
  mutate(Start.Date = as.Date(Start.Date))

grouped_data <- data %>%
  group_by(Start.Date, Disaster.Group) %>%
  summarise(Count = n()) %>%
  ungroup()

ggplot(grouped_data, aes(x = Start.Date, y = Count, color = Disaster.Group, group = Disaster.Group)) +
  geom_line(size = 1.2, alpha = 0.8) +
  geom_point(size = 2, shape = 21, fill = "white", stroke = 1) +
  geom_smooth(method = "loess", se = FALSE, aes(group = 1), 
              color = "black", linetype = "dashed", size = 0.8) +
  labs(
    title = 'Zmiana liczby katastrofw podziale na ich typ w czasie',
    subtitle = 'Liczba zdarzeń w dwóch grupach w zależności od daty',
    x = 'Data',
    y = 'Liczba zdarzeń',
    color = 'Grupa katastrofy'
  ) +
  scale_color_brewer(palette = "Set2") +
  scale_x_date(
    date_breaks = "5 years",
    date_labels = "%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 11),
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 13, hjust = 0.5, color = "grey50"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    panel.grid = element_blank(), 
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA)
  ) +
  coord_cartesian(clip = "off", expand = FALSE)

```

**Wnioski do wizualizacji:**

-   Widzimy ogólnie rzecz biorąc spadek występowania katastrof od począteku XXI wieku, lecz nasila się w obecnych czasach liczebność katastrof naturalnych. Być może, jest to spowodowane tym, że przez wiekszość czasu rozwijano przemysł technologiczny i zanieczyszczano planetę nie patrząc na skutki, i teraz gdy mamy wiele regularyzacji na temat funkcjonowania przedsiębiorstw, zostaje problem środowiska które zostało zanieczyszczone wcześniej.

**Liczba katastrof w każdym państwie**

```{r}
world <- ne_countries(scale = "medium", returnclass = "sf")
data_summary <- data %>%
  group_by(Country) %>%
  summarise(Disaster_Count = n())
world_data <- world %>%
  left_join(data_summary, by = c("name" = "Country"))
world_data$Disaster_Count[is.na(world_data$Disaster_Count)] <- 0
leaflet(world_data) %>%
  addTiles() %>%
  addPolygons(
    fillColor = ~colorNumeric(palette = "YlOrRd", domain = world_data$Disaster_Count)(Disaster_Count),
    weight = 0.5,
    opacity = 1,
    color = "black",
    fillOpacity = 0.7,
    popup = ~paste0("<b>Country:</b> ", name, 
                    "<br><b>Disaster Count:</b> ", Disaster_Count)
  ) %>%
  addLegend(
    "bottomright",
    pal = colorNumeric(palette = "YlOrRd", domain = world_data$Disaster_Count),
    values = world_data$Disaster_Count,
    title = "Disaster Count",
    opacity = 1
  )

```

**Wnioski do wizualizacji:**

-   Sumarycznie w Chinach dochodzi do największej liczby katastrof na przestrzeni lat. Po dokładnym zagłębieniu się w ten temat doszedłem do wniosku, że jest spowodowane położeniem tego państwa, przez co państwa azjatyckie (położene w sąsdiedztwie Chin) narażone są na wiele niebezpieczeństw, wysoka urbanizacja, aktywność sejsmiczna etc.

-   Ciekawym wnioskiem jest fakt że trzy państwa z największą liczbą katastrof to Chiny, USA, Indie. Jak powszechnie wiadomo, są to państwa z największą liczbą ludności 😉

**Przechodząc do następnej wizualizacji**

```{r}
data2 <- data %>%
  mutate(Year = as.numeric(format(as.Date(Start.Date), "%Y"))) %>%  # Wyciągnięcie roku z daty
  group_by(Region, Year, Disaster.Subtype) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  arrange(Region, Year, desc(Count)) %>%
  group_by(Region, Year) %>%
  slice_max(Count, n = 1)  
unique_subtypes <- unique(data2$Disaster.Subtype)
colors <- scales::hue_pal()(length(unique_subtypes))
color_mapping <- setNames(colors, unique_subtypes)
ggplot(data2, aes(x = Year, y = Region, size = Count, color = Disaster.Subtype)) +
  geom_point(alpha = 0.6) +  
  scale_size(range = c(3, 15)) +  
  scale_color_manual(values = color_mapping) +  
  labs(
    title = "Katastrofy vs Czas vs Region",
    x = "Rok",
    y = "Region",
    color = "Typ katastrofy",
    size = "Liczba wystąpień"
  ) +
  theme_minimal(base_size = 18) +
  theme(
    plot.title = element_text(size = 13, face = "bold", hjust = 0.5),
    legend.position = "left",
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9),
    axis.title = element_text(size = 10),
    axis.text = element_text(size = 9),
    panel.grid = element_blank(),
    plot.background = element_rect(fill = "white", color = NA)
  )

```

**Wnioski z wizualizacji:**

-   Oceania

    -   Dominują przez większośc czasu niszczące cyklony tropikalne, informacja może być pomocna dla architektów którzy planuja będą musieli budować konstrukcje solidniejsze na tego typu kataklizmy + oczywiście dla firm ubezpieczeniowych

-   Europa

    -   Można wnioskować że mieszkańcy Europy na przestrzeni lat mieli spore problemy z powodziami lecz, aktualnie przez zmiany klimatu najbardziej doskierwają niszczące upały.

-   Azja

    -   Widzimy jak następowały po sobie konkretne fale typów katastrof, dość jednostajni, największym problemem Azji jest prawidłowe przystosowanie się do walki z katastrofami wodnymi. Może to być pomysł na startup z pretekstem wyjazdy do Azji :)

-   Ameryki i Afryka

    -   Podobnie jak Azja, ze względu na podobne strefy klimatyczne, długośc i szerkośc geograficzną

*Poniżej przedstawiono animowany wykres z liczbą ofiar katastrof na przestrzeni lat*

```{r}
top_5_deaths <- data %>%
  filter(Disaster.Subtype != "Tsunami") %>%  # Usunięcie Tsunami
  group_by(Disaster.Subtype) %>%
  summarise(Total.Deaths = sum(Total.Deaths, na.rm = TRUE)) %>%
  top_n(5, Total.Deaths) %>%
  arrange(desc(Total.Deaths))
data_filtered <- data %>%
  filter(Disaster.Subtype %in% top_5_deaths$Disaster.Subtype, Disaster.Subtype != "Tsunami")
data_filtered <- data_filtered %>%
  mutate(Year = as.integer(format(Start.Date, "%Y"))) %>%  
  group_by(Disaster.Subtype, Year) %>%
  summarise(Total.Deaths = sum(Total.Deaths, na.rm = TRUE), .groups = 'drop') %>%
  arrange(Disaster.Subtype, Year) %>%
  group_by(Disaster.Subtype) %>%
  mutate(Cumulative.Deaths = cumsum(Total.Deaths)) %>%
  ungroup()

p <- ggplot(data_filtered, aes(x = reorder(Disaster.Subtype, Cumulative.Deaths), y = Cumulative.Deaths, fill = Disaster.Subtype)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_fill_manual(values = c("#4A4A4A", "#6B6B6B", "#8C8C8C", "#A0A0A0", "#B3B3B3")) +  
  labs(
    title = "Liczba ofiar wybranych katastrof - {frame_time}",
    x = "Typ katastrofy",
    y = "Łączna liczba ofiar"
  ) +
  theme_minimal(base_size = 14) +  
  theme(
    plot.title = element_text(size = 16, face = "bold", color = "grey30", hjust = 0.5),
    axis.title = element_text(size = 14, color = "grey30"),
    axis.text = element_text(size = 12, color = "grey50"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major = element_line(color = "grey80", size = 0.5), 
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "#F5F5F5", color = NA),  
    panel.background = element_rect(fill = "#F5F5F5", color = NA), 
    legend.position = "none"
  ) +
  transition_time(Year) +  
  ease_aes('linear') + 
  enter_fade() + 
  exit_fade() +
  transition_time(Year) +
  shadow_mark()

p

```

**Wnioski z wizualizacji:**

-   Nawiązując do poprzednich analiz wiemy że Chiny mają najwięcej katastrof, i właśnie przez takie położenie geograficzne mają wysoką aktywnośc sejsmiczną, co odzwierciedlenie w dużej liczbie zgonów spowodowanych trzęsieniami ziemi

Co Chiny mogą zrobić lepiej?

Zainwestować w AI służące do prognozowania trzęsień ziemi (Koło ATLAS może takie zrobić 😉) i rozważyć zmianę budżetu państwa z nastawieniem na lepsze systemy ostrzegania przed trzesieniem ziemi, i w pomoc medyczną osobom poszkodowanym

*Poniżej przedstawiono wykres z średnim czasem trwania katastrof z podziałem na ich podtyp*

```{r}
data$Start.Date <- as.Date(data$Start.Date, format = "%Y-%m-%d")
data$End.Date <- as.Date(data$End.Date, format = "%Y-%m-%d")
data$Duration <- as.numeric(data$End.Date - data$Start.Date)

summary_data <- data %>%
  group_by(Disaster.Subgroup, Region) %>%
  summarise(Average_Duration = mean(Duration, na.rm = TRUE)) %>%
  ungroup()

ggplot(summary_data, aes(x = Disaster.Subgroup, y = Average_Duration, fill = Region)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  labs(
    title = "Średni czas trwania katastrof według podgrupy i kontynentu",
    x = "Podgrupa katastrof",
    y = "Średni czas trwania (dni)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set3") 
```

**Wnioski:**

-   Średnio 2/3 roku Afryka jest dotknięta katastrofami, ich liczebnośc na podstawie wiemy że nie jest aż tak liczna, ale jeśli już się coś zjawi to trwa bardzo długo :(

-   Istnieje co najmniej jedna katastrofa która dotknełą Ameryki na 1/3 roku!

-   Najtrudniejsze do zwalczenia są katastrofy klimatyczne, na które możemy miecć tylko wpływ zapobiegawczy

-   W Europie najdłuższa katastrofa trwała 20 dni i była biologiczna

-   Grupą najdłużej trwających katastrof są klimatyczne katastrofy

-   Europa nie dość że nie ma wielu katastrof to jescze przechodzą dość szybko

## Statystyki dla zbioru

**Korelacje pomiędzy danymi**

```{r}
data_copy <- data %>%
  select(-Classification.Key, -Disaster.Subtype, -Disaster.Subgroup,
         -Disaster.Type, -Country, -Start.Date, -End.Date, 
         -Latitude, -Longitude)
```

```{r}
library(caret)
library(tidyverse)
dummy_vars <- dummyVars(~ Disaster.Group + Region, data = data_copy)
data_dummies <- data.frame(predict(dummy_vars, newdata = data_copy))
final_data <- cbind(data_copy[, !names(data_copy) %in% colnames(data_dummies)], data_dummies)
final_data <- final_data %>% 
  select(-Disaster.Group,  -Region)
 new_column_names <- c("Deaths", "Injured", "Affected","Damage", "Natural", "Tech", "Africa", "America", "Asia","Europe","Oceania")
colnames(final_data) <- new_column_names

```

```         
```

```{r}
library(corrplot)
options(repr.plot.width = 14, repr.plot.height = 10)
cor_matrix <- cor(final_data)
cor_matrix <- round(cor_matrix, 2)
corrplot(cor_matrix, 
         method = "circle", 
         type = "lower", 
         col = colorRampPalette(c("#2166AC", "white", "#B2182B"))(200), 
         title = "Macierz Korelacji pomiędzy zmiennymi", 
         diag = FALSE, 
         tl.col = "black", 
         tl.srt = 45, 
         number.cex = 0.8, 
         addCoef.col = "black", 
         cl.cex = 1, 
         tl.cex = 1.2, 
         mar = c(0, 0, 2, 0))
```

**Wnioski na temat korelacji:**

-   Zmienne nie wykazują między sobą znaczących, czy też zaskakujących korelacji, wykazują oczywistę zależność pomiędzy liczbą ofiar a zgonami, co wydaje się dosyć oczywiste, tam samo zależność na temat kosztów

**ANOVA dla liczby śmiertelnych przypadków dla każdego typu katastrofy**

Wykonuję ANOVA ponieważ są spełnione założenia, rozkład liczby śmierci po odsianiu outlierów do poziomu IQR, jest zbliżony do normalnego, wariancje w porównywanych grupach są podobne etc.

Czym jest ANOVA?

Jest to metoda statystyczna używana do sprawdzania, czy średnie w więcej niż dwóch grupach są istotnie różne. Zakłada, że dane w grupach pochodzą z rozkładu normalnego o tej samej wariancji.

$$
F = \frac{\text{MS}_{\text{between}}}{\text{MS}_{\text{within}}}\\
$$

$$
\text{MS}_{\text{between}} = \frac{\text{SS}_{\text{between}}}{k-1} – średnia ~kwadratów ~między~ grupami.  
$$

$$
\\\text{MS}_{\text{within}} = \frac{\text{SS}_{\text{within}}}{N-k} – średnia ~kwadratów ~wewnątrz ~grup.
$$

**Kluczowe założenie**: porównujemy wariancję między grupami do wariancji wewnątrz grup.

```{r}
data$Region <- as.factor(data$Disaster.Type)
anova_result <- aov(Total.Deaths ~ Disaster.Type, data = data)
summary(anova_result)
```

**Wnioski:**

-   Test wskazuje istotne różnice pomiędzy typami katastrof.

-   Wysoka wartość F value (statystyka testowa ANOVA) wskazuje na większe różnice między grupami w porównaniu do różnic wewnątrz grup.

-   Bardzo mała p-wartość (\< 0.001) oznacza, że różnice w średniej liczbie śmiertelnych przypadków pomiędzy różnymi typami katastrof są statystycznie istotne.

```         
```

**Test Chi-Kwadrat zależnośći kontynentu a typem katastrofy**

Czym jest test Chi-Kwadrat?

Test chi-kwadrat służy do analizy zależności między zmiennymi kategorycznymi poprzez porównanie obserwowanych i oczekiwanych wartości.

$$
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
$$

$$
O_i- obserwowana ~wartość ~w ~i-tej ~kategorii
$$

$$
 E_i-oczekiwana~ wartość ~w ~i-tej ~kategorii.
$$

```{r}
table_region_type <- table(data$Region, data$Disaster.Type)
chisq.test(table_region_type)
```

**Wnioski:**

-   bardzo niska p value wskazuje na silną zależność między regionem a typem katastrofy, co potwierdza wcześniej stawiane wnioski przy wykresie bąbelkowym, więc odrzucamy hipotezę o niezależności regionu od typu katastrofy

W przeciwieństwie do liczby ofiar śmiertelnych, typy katastrof są zależne od kontynety, co wydaje się dosyć intuicyjne na podstawie położenia w innych strefach klimatycznych etc.

**Test Kruskala-Wallisa dla liczby zgonów w zależności od kontynentu**

Czym jest test K-W?

Test Kruskala-Wallisa to taki nieparametryczny odpowiednik ANOVA, stosowany, gdy dane nie spełniają założeń normalności. Analizuje mediany zamiast średnich.

$$
H = \frac{12}{N(N+1)} \sum_{i=1}^{k} \frac{R_i^2}{n_i} - 3(N+1)
$$

$$
R_i-suma ~rang~ w ~i-tej ~grupie
$$

$$
n_i- liczba ~obserwacji ~w ~i-tej~ grupie
$$

$$
N- całkowita ~liczba ~obserwacji
$$

```{r}
kruskal.test(Total.Deaths ~ Region, data = data)

```

**Wnioski:**

-   Bardzo mała p-wartość wskazuje na wysoce statystycznie istotne różnice między regionami pod względem liczby zgonów. Dokładnie pomiędzy różnicami median.

Wszystkie testy miały na celu potwierdzić poprzednie wnioski na podstawie wizualizacji. Istnieje możliwość wykonania jeszcze testów post-hoc Dunn\`a w celu dalszej analizy, lecz nie na tym miał się skupiać projekt więc statystyczną część projektu uważam za skończoną.

## Uczenie maszynowe

### PCA

**Uczenie nienadzorowane - analiza gównych składowych**

Czym jest PCA?

PCA to metoda redukcji wymiarowości, która transformuje dane na nową przestrzeń z osiami maksymalizującymi wariancję. Wynikiem są główne składowe.

Wzór na wariancję wyjaśnioną przez główną składową:

$$
\text{Variance Explained} = \frac{\lambda}{\sum \lambda}
$$

$$
\lambda: wartość ~własna ~macierzy~ kowariancji ~lub ~korelacji
$$

```{r}
library(ggplot2)
library(reshape2)
library(scales)
pca <- prcomp(final_data[, sapply(final_data, is.numeric)], center = TRUE, scale. = TRUE)
loadings <- pca$rotation * sqrt(pca$sdev)
explained_variance <- pca$sdev^2 / sum(pca$sdev^2) * 100
loadings_data <- as.data.frame(loadings[, 1:4])
colnames(loadings_data) <- paste("PC", 1:4, sep = "")
loadings_data$Feature <- rownames(loadings_data)
loadings_long <- melt(loadings_data, id.vars = "Feature", variable.name = "PC", value.name = "Loading")
ggplot(loadings_long, aes(x = Feature, y = Loading, fill = PC)) +
  geom_bar(stat = "identity", position = "dodge", color = "black") +
  facet_wrap(~PC, scales = "free_y", ncol = 2) +
  labs(title = "PCA Loadings for First 4 Principal Components", x = "Feature", y = "Loading Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        strip.text = element_text(face = "bold"),
        panel.grid.major = element_line(color = "gray", linetype = "dashed")) +
  scale_fill_manual(values = c("#1C3041", "#9B1D20", "#0B6E4F", "#895884")) +
  geom_text(aes(label = sprintf("%.2f", Loading), 
                vjust = ifelse(Loading > 0, -0.3, 1.5)), 
            size = 3)
```

**Wnioski na temat analizy głównych składowych:**

-   Analiza składowych głównych wskazuje, że regiony oraz typ katastrof (naturalne vs technologiczne) silnie różnicują dane.
-   Deaths i Injured to główne zmienne, które wyjaśniają intensywność katastrof w ramach PC3.
-   Poszczególne regiony (np. Europe w PC1 i Africa w PC4) mają różny wpływ na wyniki, co podkreśla ich specyficzne cechy w analizowanych danych.

**Uczenie nadzorowane - klasyfikacja katastrofa Naturalna VS Technologiczna**

*Z uwagi na niezbalansowaną klasą posłużymy się metodą upsamplingu - SMOTE*

*Metoda ta bazując na podobieństwie cosinusowym tworzy syntetyczne próbki klasy mniejszościowej*

```{r}
#write.csv(final_data, "final_data.csv",row.names = F)
```

```{python}
# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, roc_auc_score
# from imblearn.over_sampling import SMOTE
# import pandas as pd
# 
# final_data = pd.read_csv("final_data.csv")
# X = final_data.drop(columns=["Natural","Tech"])
# y = final_data["Natural"]
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# smote = SMOTE(random_state=42)
# X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
# train_balanced = pd.concat([X_train_balanced, y_train_balanced], axis=1)
# train_balanced.to_csv("train_balanced.csv", index=False)
# test_data = pd.concat([X_test, y_test], axis=1)
# test_data.to_csv("test_data.csv", index=False)
```

### Drzewo decyzyjne

```{r}
library(rpart)
library(rpart.plot)
library(caret)
library(rpart)
library(rpart.plot)
library(caret)
library(pROC)
train_balanced <- read.csv("train_balanced.csv")
test_data <- read.csv("test_data.csv")
train_balanced$Natural <- as.factor(train_balanced$Natural)
test_data$Natural <- as.factor(test_data$Natural)
tree_model <- rpart(Natural ~ ., data = train_balanced, method = "class")
rpart.plot(tree_model, 
           type = 2, 
           extra = 104, 
           box.palette = "Greens", 
           shadow.col = "gray", 
           main = "Drzewo Decyzyjne po SMOTE")
tree_predictions <- predict(tree_model, test_data, type = "class")
tree_conf_matrix <- confusionMatrix(tree_predictions, test_data$Natural)
tree_accuracy <- tree_conf_matrix$overall["Accuracy"]
tree_prob_predictions <- predict(tree_model, test_data, type = "prob")[, 2]
roc_curve_tree <- roc(test_data$Natural, tree_prob_predictions)
tree_auc <- auc(roc_curve_tree)

```

Jak działa drzewo decyzyjne?

Opiera się na podziałach danych w celu maksymalizacji "czystości" węzłów.

Wzór na "czystość" węzła, tzw indeks Giniego

$$
G = 1 - \sum_{i=1}^{k} p_i^2\\
$$

$$
p_i- proporcja~obserwacji~ należących~ do~ klasy~i
$$

### Regresja logistyczna

```{r}
library(caret)
library(ggplot2)
library(pROC)
X_train <- train_balanced[, !(names(train_balanced) %in% c("Natural"))]
y_train <- train_balanced$Natural

X_test <- test_data[, !(names(test_data) %in% c("Natural"))]
y_test <- test_data$Natural
logit_model <- glm(y_train ~ ., data = data.frame(X_train, y_train), family = binomial)
logit_pred <- predict(logit_model, newdata = data.frame(X_test), type = "response")
logit_pred_class <- ifelse(logit_pred > 0.5, 1, 0)
logit_conf_matrix <- confusionMatrix(factor(logit_pred_class), factor(y_test))
logit_accuracy <- logit_conf_matrix$overall["Accuracy"]
roc_curve_logit <- roc(y_test, logit_pred)
plot(roc_curve_logit, main = "Krzywa ROC dla regresji logistycznej", col = "blue", lwd = 2)

logit_auc <- auc(roc_curve_tree)
```

Co to takiego ta straszna regresja logistyczna?

Regresja logistyczna to metoda modelowania prawdopodobieństwa przynależności do określonej klasy. Wykorzystuje funkcję logistyczną tzw. sigmoidalną :D

$$
P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p)}}
$$

### Las losowy

```{r}
library(randomForest)
rf_model <- randomForest(y_train ~ ., data = data.frame(X_train, y_train))
rf_pred <- predict(rf_model, newdata = data.frame(X_test))
rf_conf_matrix <- confusionMatrix(factor(rf_pred), factor(y_test))
rf_accuracy <- rf_conf_matrix$overall["Accuracy"]
roc_curve_rf <- roc(y_test, as.numeric(rf_pred))
plot(roc_curve_rf, main = "Krzywa ROC dla lasu losowego", col = "red", lwd = 2)

rf_auc <- auc(roc_curve_rf)
```

Jak działa las losowy?

Las losowy to zespół drzew decyzyjnych, zgodnie z intuicją - kilka drzew tworzy las :) który dokonuje predykcji dzięki agregacji wyników wielu drzew (np. głosowanie większościowe).

$$
y' = majority~_~vote(T_1(x), T_2(x), \dots, T_m(x))
$$

$$
T_m(x): wynik~ m-tego~ drzewa
$$

Czym są pokazane wyżej krzywe ROC (Reciver Operating Characteristic)

najpierw wprowadźmy kilka definicji pomocniczych

-   Czułośc:

    $$
     \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
    $$

-   Specyficznośc

    $$
    \text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}
    $$

-   Odsetek fałszywie pozytywnych tzw. FPR

    $$
     \text{FPR} = 1 - \text{Specificity} = \frac{\text{FP}}{\text{FP} + \text{TN}}
    $$

-   AUC - jest to pole pod krzywą (przydają się jednak te całki z analizy co nie :D?) które nam mówi o

    dokładności klasyfikatora, na podstawie wcześniej podanych metryk

Warto dodać że przy ewaluacji modeli klasyfikacyjnych predykujących klasy niezbalansowane, warto jest zwrócić uwagę na takie metryki jak F1 score oraz Cohen Kappa :D

### Podsumowanie zbudowanych modeli

```{r}
library(reshape2)
comparison_data <- data.frame(
  Model = c("Regresja logistyczna", "Las losowy", "Drzewo decyzyjne"),
  Accuracy = c(logit_accuracy, rf_accuracy, tree_accuracy),
  AUC = c(logit_auc, rf_auc, tree_auc)
)
comparison_data_long <- melt(comparison_data, id.vars = "Model", 
                             variable.name = "Metric", value.name = "Value")
ggplot(comparison_data_long, aes(x = Model, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
  geom_text(aes(label = round(Value, 2)), 
            position = position_dodge(width = 0.7), vjust = -0.3, size = 4) +
  scale_fill_manual(values = c("skyblue", "orange"), 
                    labels = c("Accuracy", "AUC")) +
  labs(
    title = "Drzewo Decyzyjne vs Las losowy vs Regresja Logistyczna",
    x = "Model",
    y = "Wartość",
    fill = "Metryka"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "top",
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12)
  ) +
  coord_cartesian(ylim = c(0, 1.1))
```

**Wnioski:**

-   Modele drzewa decyzyjnego oraz lasu losowego wykazały wysoką skuteczność w klasyfikacji podstawowych kategorii katastrof. Dzięki analizie dendrogramu można łatwo zidentyfikować kluczowe predyktory, co czyni te modele nie tylko dokładnymi, ale również interpretowalnymi. Las losowy, jako bardziej zaawansowany model, może lepiej radzić sobie z problemami związanymi z nadmiernym dopasowaniem w porównaniu z pojedynczym drzewem.

-   Regresja logistyczna, choć ceniona za prostotę i możliwość interpretacji współczynników, nie poradziła sobie złożonością problemu. Niski wynik AUC sugeruje, że model nie jest w stanie dobrze rozróżniać kategorii katastrof na podstawie dostępnych danych, co wskazuje na ograniczoną zdolność do generalizowania w tym konkretnym kontekście.

-   Wyniki modeli sugerują, że mogłyby one być użyteczne w praktycznych sytuacjach, np. w przypadku służb ratunkowych. Mając ograniczone informacje, takie jak liczba rannych czy ofiar śmiertelnych, system oparty na tych modelach mógłby sugerować odpowiedni typ katastrofy. Na tej podstawie można by efektywnie alokować zasoby i szybko wezwać odpowiednie jednostki ratunkowe.

## Podsumowanie

Odnosząc się do pytań postawionych sobie na samym początku projektu jesteśmy w stanie podać odpowiedźi na nurtujące nas pytania

-   **Które państwo lub region jest najbezpieczniejszy do życia pod względem występowania katastrof?**

    -   Państwa Europejskie cechują się największą bezpiecznością pod względem liczebności katastrof, więc w Polsce jesteśmy póki co bezpieczni

-   **Które państwo lub region jest najbardziej narażone na występowanie katastrof?**

    -   Są to Chiny, mimo wysokie poziomu życia i rozwiniętych technologi są narażone na duże niebezpieczeństwo z strony zagrożeń naturalnych i technologicznych

-   **Jakie typy katastrof najczęściej występują w określonych regionach?**

    -   Europa - aktualnie możemy się borykać z falami ciepła - upałami

    -   Oceania - niezmiennie tropikalne burze

    -   Azja, Ameryki, Afryka - aktualnie może mieć problemy z powodziami

-   **Jakie są skutki katastrof dla mieszkańców poszczególnych regionów?**

    -   Duża ilośc śmiertelnych ofiar w krajach narażonych na większą aktywność sejsmiczną

    -   Straty finansowe związane głównie z tropikalnymi burzami

Mam nadzieje że po przeczytaniu tego raportu zrozumiemy w jakim stanie jest nasza planeta i będziemy żyli z świadomością działania w kierunku zapobiegania katastrofom

## Dalsze kroki...

Projekt ten ma znaczący potencjał więc w przyszłości planuje zrobić aplikację w Streamlitcie do której podepnę model wielowyjściowy który będzie w stanie przewidywać na podstawie konkrentych predyktorów dokładnie czas trwania katastrofy, miejsce, skutki etc. na podstawie intregacji z bazami danych NASA, ewentualnie kilka modeli odpowiedzialnych za grupy predykcji.

![](images/clipboard-2638264063.png)
